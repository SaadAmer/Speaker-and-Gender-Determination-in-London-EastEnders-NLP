{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Combined.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioUK8KcVQOSB",
        "colab_type": "code",
        "outputId": "ce78c0e6-eee2-4805-fc71-f03ee3e98214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install unicodecsv\n",
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import unicodecsv                               # csv reader\n",
        "import re                                       # regular expressions\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "\n",
        "# To do preprocessing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
        "import numpy as np # To compute the average results\n",
        "\n",
        "from random import shuffle # To shuffle the dataset\n",
        "\n",
        "\n",
        "# To use feature selection in the Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "nltk.download('large_grammars')\n",
        "grammar = nltk.data.load('grammars/large_grammars/atis.cfg')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unicodecsv\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp27-none-any.whl size=10771 sha256=ab66c32a4f948dfc8bd341444f630a060db08ed63d7f348c37669854e498c2b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv\n",
            "Successfully installed unicodecsv-0.14.1\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python2.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python2.7/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python2.7/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python2.7/dist-packages (from gensim) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python2.7/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python2.7/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python2.7/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python2.7/dist-packages (from smart-open>=1.2.1->gensim) (1.9.189)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python2.7/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python2.7/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.189)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python2.7/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python2.7/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from s3transfer<0.3.0,>=0.2.0->boto3->smart-open>=1.2.1->gensim) (3.2.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package large_grammars to /root/nltk_data...\n",
            "[nltk_data]   Unzipping grammars/large_grammars.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJBiUi6KQTUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data from the files and append it\n",
        "\n",
        "def populate(path, reviewText=None):\n",
        "    with open(path, 'rb') as f:\n",
        "        reader = unicodecsv.reader(f, encoding='utf-8', delimiter=',')\n",
        "        # print(reader)\n",
        "        reader.next()\n",
        "        for line1 in reader:\n",
        "            if line1[1] not in labelMap1:\n",
        "                # print(line1[1])\n",
        "                labelMap1[line1[1]] = line1[1]\n",
        "################## FOR GETTING GENDER FROM LINES LOAD INTO rawData (training) ########\n",
        "\n",
        "def loadData(path, reviewText=None):\n",
        "    with open(path, 'rb') as f:\n",
        "        reader = unicodecsv.reader(f, encoding='utf-8', delimiter=',')\n",
        "        # print(reader)\n",
        "        reader.next()\n",
        "        \n",
        "        for line in reader:\n",
        "            # if line[1] not in labelMap1:\n",
        "            #   labelMap1[line[1]] = line[1]\n",
        "            # print(line)\n",
        "            # (reviewText, label) = parseReview(line)\n",
        "            # rawData.append((reviewText, label))\n",
        "            (Text, Label) = parseReviewImproved(line)\n",
        "            rawData.append((Text, Label))\n",
        "\n",
        "############## FOR GETTING GENDER FROM LINES LOAD INTO rawData1 (testing)########\n",
        "def loadData1(path, reviewText=None):\n",
        "    with open(path, 'rb') as f:\n",
        "        reader = unicodecsv.reader(f, encoding='utf-8', delimiter=',')\n",
        "        # print(reader)\n",
        "        reader.next()\n",
        "        for line in reader:\n",
        "            # print(line)\n",
        "            # (reviewText, label) = parseReview(line)\n",
        "            # rawData.append((reviewText, label))\n",
        "            (Text, Label) = parseReviewImproved(line)\n",
        "            rawData1.append((Text, Label))\n",
        "\n",
        "######################### FOR GETTING SPEAKER, LOADING INTO rawData2 (TRAINING) ################\n",
        "def loadData2(path, reviewText=None):\n",
        "    with open(path, 'rb') as f:\n",
        "        reader = unicodecsv.reader(f, encoding='utf-8', delimiter=',')\n",
        "        # print(reader)\n",
        "        reader.next()\n",
        "        for line in reader:\n",
        "            # print(line)\n",
        "            # (reviewText, label) = parseReview(line)\n",
        "            # rawData.append((reviewText, label))\n",
        "            (Text, Label) = parseReviewImproved1(line)\n",
        "            rawData2.append((Text, Label))\n",
        "\n",
        "\n",
        "######################### FOR GETTING SPEAKER, LOADING INTO rawData3 (Testing) ################\n",
        "def loadData3(path, reviewText=None):\n",
        "    with open(path, 'rb') as f:\n",
        "        reader = unicodecsv.reader(f, encoding='utf-8', delimiter=',')\n",
        "        # print(reader)\n",
        "        reader.next()\n",
        "        for line in reader:\n",
        "            # print(line)\n",
        "            # (reviewText, label) = parseReview(line)\n",
        "            # rawData.append((reviewText, label))\n",
        "            (Text, Label) = parseReviewImproved1(line)\n",
        "            rawData3.append((Text, Label))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def splitData(percentage):\n",
        "#     dataSamples = len(rawData)\n",
        "#     halfOfData = int(len(rawData)/2)\n",
        "#     trainingSamples = int((percentage*dataSamples)/2)\n",
        "#     for (Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
        "#         trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
        "#     for (Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
        "#         testData.append((toFeatureVector(preProcess(Text)),Label))\n",
        "\n",
        "\n",
        "####### FOR GETTING GENDER FROM LINES ############################\n",
        "\n",
        "def splitData(num):\n",
        "    for (Text, Label) in rawData:\n",
        "        trainData.append((toFeatureVector(preProcess(Text), num),Label))\n",
        "\n",
        "def splitData1(num):\n",
        "    for (Text, Label) in rawData1:\n",
        "        testData.append((toFeatureVector(preProcess(Text), num),Label))\n",
        "\n",
        "####### FOR GETTING SPEAKER FROM LINES ###########################\n",
        "def splitData2(num):\n",
        "    for (Text, Label) in rawData2:\n",
        "        trainData1.append((toFeatureVector(preProcess(Text), num),Label))\n",
        "\n",
        "def splitData3(num):\n",
        "    for (Text, Label) in rawData3:\n",
        "        testData1.append((toFeatureVector(preProcess(Text), num),Label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkphvQ76QWuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the output classes\n",
        "maleLabel = 'male'\n",
        "femaleLabel = 'female'\n",
        "labelMap = {'male' : maleLabel, 'female' : femaleLabel}\n",
        "# labelMap1 = {}\n",
        "\n",
        "# Convert line from input file into an id/text/label tuple plus meta features\n",
        "def parseReviewImproved(reviewLine):\n",
        "    # Id    = int(reviewLine[0])\n",
        "    # Rating = int(reviewLine[2])\n",
        "    # VerifiedPurchase = reviewLine[3]\n",
        "    # Category = reviewLine[4]\n",
        "    Text  = reviewLine[0]\n",
        "    Label = labelMap[reviewLine[2]]\n",
        "    return (Text, Label)\n",
        "################## FOR GETTING SPEAKER #########################\n",
        "def parseReviewImproved1(reviewLine):\n",
        "    # Id    = int(reviewLine[0])\n",
        "    # Rating = int(reviewLine[2])\n",
        "    # VerifiedPurchase = reviewLine[3]\n",
        "    # Category = reviewLine[4]\n",
        "    Text  = reviewLine[0]\n",
        "    Label1 = labelMap1[reviewLine[1]]\n",
        "    return (Text, Label1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJvjsroXQeK_",
        "colab_type": "code",
        "outputId": "1ba70501-d52a-4f3e-eb7b-62e0b6eb9468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
        "\n",
        "# input: a string of one review\n",
        "def preProcess(text):\n",
        "    # should return a list of tokens\n",
        "    \n",
        "    # word tokenisation, including punctuation removal\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = pos_tag(tokens)\n",
        "    \n",
        "    # lowercasing\n",
        "    tokens = [(t.lower(),p) for t,p in tokens]\n",
        "\n",
        "    # stopword removal- benefits are it removes rare words, though bad for bigram relations\n",
        "    if False:\n",
        "        stop = set(stopwords.words('english'))\n",
        "        tokens = [(t,p) for t,p in tokens if t not in stop]\n",
        "    \n",
        "    # lemmatisation\n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    tokens = [(lemmatiser.lemmatize(t),p) for t,p in tokens]\n",
        "    tokens = [(t,p) for t,p in tokens if t] # ensure no empty space\n",
        "    toks = [t for t,p in tokens]\n",
        "\n",
        "    # tokens = [pos_tag(word) for word in tokens]\n",
        "    # print(tokens[0], pos_tag(tokens[0]))\n",
        "    # print(pos_tag(tokens))\n",
        "\n",
        "    # tokens = pos_tag(tokens)\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "print(preProcess(\"hello this is the, ehh... presumably, a crying situations!\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('hello', 'NN'), ('this', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('ehh', 'NN'), ('presumably', 'RB'), ('a', 'DT'), (u'cry', 'NN'), (u'situation', 'NNS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZEmjp4ReAD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEXT PREPROCESSING AND FEATURE VECTORIZATION PART 2 (WORD2VEC)\n",
        "def gen_train_lib(num):\n",
        "    if num == 1:\n",
        "        for (Text, Label) in rawData:\n",
        "            temp1 = gen_helper(Text,num)\n",
        "        # genderf1 = 1\n",
        "\n",
        "    elif num == 2:\n",
        "        for (Text, Label) in rawData1:\n",
        "            temp1 = gen_helper(Text, num)\n",
        "        # genderf2 = 1\n",
        "    \n",
        "                                   \n",
        "def gen_helper(text, num):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = pos_tag(tokens)\n",
        "    \n",
        "    # lowercasing\n",
        "    tokens = [(t.lower(),p) for t,p in tokens]\n",
        "\n",
        "    # stopword removal- benefits are it removes rare words, though bad for bigram relations\n",
        "    if False:\n",
        "        stop = set(stopwords.words('english'))\n",
        "        tokens = [(t,p) for t,p in tokens if t not in stop]\n",
        "    \n",
        "    # lemmatisation\n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    tokens = [(lemmatiser.lemmatize(t),p) for t,p in tokens]\n",
        "    tokens = [(t,p) for t,p in tokens if t] # ensure no empty space\n",
        "    toks = [t for t,p in tokens]\n",
        "\n",
        "    if num == 1:\n",
        "      gender_whole_train.append(toks)\n",
        "    else:\n",
        "      gender_whole_test.append(toks)\n",
        "\n",
        "def gen_train():\n",
        "    gender_model_test = Word2Vec(gender_whole_test, min_count=1)\n",
        "def gen_test():    \n",
        "    gender_model_train = Word2Vec(gender_whole_train, min_count=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWBdWIPNQhey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# QUESTION 2\n",
        "featureDict = {} # the global feature dictionary\n",
        "\n",
        "def toFeatureVector(tokens, num):\n",
        "    # return a dictionary 'featureVect' where the keys are the tokens in 'words' and the values are the number of occurrences of the tokens\n",
        "    # start by using binary values only:\n",
        "#     baseDict = {}\n",
        "    featureVec = {}\n",
        "    prev_tag = {}\n",
        "\n",
        "    for w, pos in tokens:\n",
        "        try:\n",
        "            sum = 0\n",
        "            featureVec[w] += 1.0/len(tokens)\n",
        "            # USING POS TAGS. COMMENT OUT THE BELOW SECTION TO OBSERVE SCORE WITHOUT POS TAGS\n",
        "            if w in prev_tag:\n",
        "              if pos != prev_tag[w]:\n",
        "                featureVec[w] -= 1.0/len(tokens)\n",
        "              else:\n",
        "                featureVec[w] += 1.0/len(tokens)\n",
        "            prev_tag[w] = pos\n",
        "            ########################\n",
        "\n",
        "            ## USING WORD2VEC ###############\n",
        "            \n",
        "            temp_word = w\n",
        "            if num == 1:\n",
        "                ret_list = model1.wv.most_similar(temp_word)\n",
        "            elif num == 2:\n",
        "                ret_list = model2.wv.most_similar(temp_word)\n",
        "            length = len(ret_list)\n",
        "            for x,y in ret_list:\n",
        "                sum = sum + y\n",
        "            avg = sum / length\n",
        "            featureVec[w] += avg\n",
        "\n",
        "            ##################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        except KeyError:\n",
        "            featureVec[w] = 1.0/len(tokens)\n",
        "        try:\n",
        "            featureDict[w] += 1.0/len(tokens)\n",
        "        except KeyError:\n",
        "            featureDict[w] = 1.0/len(tokens)\n",
        "    \n",
        "    # # just get bigram binary presence or not\n",
        "    # for i in range(1, len(tokens)):\n",
        "    #     bigram = tokens[0][i-1] + \" \" + tokens[0][i]\n",
        "    #     try:\n",
        "    #         featureVec[bigram] = 1 #+= 1.0/len(tokens)\n",
        "    #     except KeyError:\n",
        "\n",
        "    #         featureVec[bigram] = 1 #= 1.0/len(tokens)\n",
        "    #     try:\n",
        "    #         featureDict[bigram] += 1.0\n",
        "    #     except KeyError:\n",
        "    #         featureDict[bigram] = 1.0\n",
        "    # print(featureVec)\n",
        "    return featureVec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3yZCBAuQlY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "def trainClassifier(trainData):\n",
        "    print \"Training Classifier...\"\n",
        "    # pipeline =  Pipeline([('tfidf', TfidfTransformer()),('chi2', SelectKBest(chi2, k=948)),('svc', LinearSVC(loss = 'hinge'))])\n",
        "    # pipeline =  Pipeline([('tfidf', TfidfTransformer()),('chi2', SelectKBest(chi2, k=1555)),('svc', LinearSVC(loss = 'hinge'))])\n",
        "    pipeline =  Pipeline([('tfidf', TfidfTransformer()),('chi2', SelectKBest(chi2, k=1555)),('svc', LinearSVC(loss = 'squared_hinge'))])\n",
        "\n",
        "\n",
        "    # print(\"INSIDE\")\n",
        "    return SklearnClassifier(pipeline).train(trainData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w2RDHPFQoHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
        "\n",
        "def predictLabels(reviewSamples, classifier):\n",
        "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
        "\n",
        "def predictLabel(text, classifier):\n",
        "    return classifier.classify(toFeatureVector(preProcess(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOQp9-7rQq15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossValidate(dataset, folds):\n",
        "    shuffle(dataset)\n",
        "    results = []\n",
        "    foldSize = len(dataset)/folds\n",
        "    \n",
        "    for i in range(0,len(dataset),foldSize):\n",
        "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
        "        print \"Fold start on items %d - %d\" % (i, i+foldSize)\n",
        "        myTestData = dataset[i:i+foldSize]\n",
        "        myTrainData = dataset[:i] + dataset[i+foldSize:]\n",
        "        # print(\"HERE\")\n",
        "        classifier = trainClassifier(myTrainData)\n",
        "        # print(\"HERE1\")\n",
        "        y_true = map(lambda x: x[1], myTestData)\n",
        "        # print(\"HERE2\")\n",
        "        y_pred = predictLabels(myTestData, classifier)\n",
        "        # print(\"HERE3\")\n",
        "        results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
        "        \n",
        "    avgResults = map(np.mean,zip(*results)[:3])\n",
        "    return avgResults"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN2Ms_JhQtbv",
        "colab_type": "code",
        "outputId": "738ede5e-b1fb-43e1-e4d8-41cc6220ec91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# MAIN\n",
        "labelMap1 = {}\n",
        "# loading reviews\n",
        "rawData = [] # the filtered data from the dataset file for the training Data GETTING GENDER\n",
        "trainData = [] # the training data  GETTING GENDER\n",
        "testData = [] # the test data  GETTING GENDER\n",
        "rawData1 = [] # the filtered data from the dataset file for the testing Data GETTING GENDER\n",
        "\n",
        "rawData2 = [] # the filtered data from the dataset file for the training Data  GETTING SPEAKER\n",
        "rawData3 = [] # the filtered data from the dataset file for the testing Data   GETTING SPEAKER\n",
        "trainData1 = [] # the training data  GETTING SPEAKER\n",
        "testData1 = [] # the test data  GETTING SPEAKER\n",
        "\n",
        "gender_whole_train = []\n",
        "gender_whole_test = []\n",
        "\n",
        "\n",
        "# references to the data files\n",
        "trainPath = '/training.csv'\n",
        "\n",
        "\n",
        "testPath = '/test.csv'\n",
        "\n",
        "# do the actual stuff\n",
        "print \"Now %d rawData, %d rawData1, %d trainData, %d testData\" % (len(rawData), len(rawData1), len(trainData), len(testData))\n",
        "print \"Preparing the dataset...\"\n",
        "\n",
        "populate(trainPath)\n",
        "loadData(trainPath)\n",
        "loadData1(testPath)\n",
        "\n",
        "loadData2(trainPath)\n",
        "loadData3(testPath)\n",
        "\n",
        "gen_train_lib(1)\n",
        "gen_train_lib(2)\n",
        "\n",
        "model1 = Word2Vec(gender_whole_train, min_count=1)\n",
        "model2 = Word2Vec(gender_whole_test, min_count=1)\n",
        "\n",
        "# print \"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData))\n",
        "print \"Now %d rawData, %d rawData1, %d trainData, %d testData\" % (len(rawData), len(rawData1), len(trainData), len(testData))\n",
        "print \"Preparing training and test data...\"\n",
        "############# FOR GETTING GENDER\n",
        "splitData(1)\n",
        "splitData1(2)\n",
        "##################### FOR GETTING SPEAKER\n",
        "\n",
        "splitData2(1)\n",
        "splitData3(2)\n",
        "# print \"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData))\n",
        "print \"Now %d rawData, %d rawData1, %d trainData, %d testData\" % (len(rawData), len(rawData1), len(trainData), len(testData))\n",
        "\n",
        "\n",
        "# print \"Now %d rawData2, %d trainData1, %d testData1\" % (len(rawData2), len(trainData1), len(testData1))\n",
        "print \"Now %d rawData2, %d rawData3, %d trainData1, %d testData1\" % (len(rawData2), len(rawData3), len(trainData1), len(testData1))\n",
        "\n",
        "\n",
        "# print(trainData)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now 0 rawData, 0 rawData1, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 10112 rawData, 1123 rawData1, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now 10112 rawData, 1123 rawData1, 10112 trainData, 1123 testData\n",
            "Now 10112 rawData2, 1123 rawData3, 10112 trainData1, 1123 testData1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsX0u8STQwy-",
        "colab_type": "code",
        "outputId": "917af0b6-c3a6-436d-dadf-7b095a5b0e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "# CROSS VALIDATION FOR GENDER\n",
        "print(\"CROSSVALIDATING FOR GENDER\")\n",
        "cv_results = crossValidate(trainData, 10)\n",
        "print cv_results\n",
        "\n",
        "# print(\"CROSSVALIDATING FOR SPEAKER\")\n",
        "# cv_results1 = crossValidate(trainData1, 10)\n",
        "# print cv_results1"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CROSSVALIDATING FOR GENDER\n",
            "Fold start on items 0 - 1011\n",
            "Training Classifier...\n",
            "Fold start on items 1011 - 2022\n",
            "Training Classifier...\n",
            "Fold start on items 2022 - 3033\n",
            "Training Classifier...\n",
            "Fold start on items 3033 - 4044\n",
            "Training Classifier...\n",
            "Fold start on items 4044 - 5055\n",
            "Training Classifier...\n",
            "Fold start on items 5055 - 6066\n",
            "Training Classifier...\n",
            "Fold start on items 6066 - 7077\n",
            "Training Classifier...\n",
            "Fold start on items 7077 - 8088\n",
            "Training Classifier...\n",
            "Fold start on items 8088 - 9099\n",
            "Training Classifier...\n",
            "Fold start on items 9099 - 10110\n",
            "Training Classifier...\n",
            "Fold start on items 10110 - 11121\n",
            "Training Classifier...\n",
            "[0.5871007803451584, 0.5404639870515241, 0.5549874771377812]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYX70lk0f6rY",
        "colab_type": "code",
        "outputId": "f61b411b-f0c2-463b-8a16-b4d6cf89bd75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "####### CROSSVALIDATION FOR SPEAKER\n",
        "print(\"CROSSVALIDATING FOR SPEAKER\")\n",
        "cv_results1 = crossValidate(trainData1, 10)\n",
        "print cv_results1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CROSSVALIDATING FOR SPEAKER\n",
            "Fold start on items 0 - 1011\n",
            "Training Classifier...\n",
            "Fold start on items 1011 - 2022\n",
            "Training Classifier...\n",
            "Fold start on items 2022 - 3033\n",
            "Training Classifier...\n",
            "Fold start on items 3033 - 4044\n",
            "Training Classifier...\n",
            "Fold start on items 4044 - 5055\n",
            "Training Classifier...\n",
            "Fold start on items 5055 - 6066\n",
            "Training Classifier...\n",
            "Fold start on items 6066 - 7077\n",
            "Training Classifier...\n",
            "Fold start on items 7077 - 8088\n",
            "Training Classifier...\n",
            "Fold start on items 8088 - 9099\n",
            "Training Classifier...\n",
            "Fold start on items 9099 - 10110\n",
            "Training Classifier...\n",
            "Fold start on items 10110 - 11121\n",
            "Training Classifier...\n",
            "[0.18154126598572554, 0.1766927434583221, 0.1564397853007664]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlE6GUGLQy2v",
        "colab_type": "code",
        "outputId": "a9ca79d0-86bd-4082-d9a2-b5b64da1677a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "##### PREDICTION FOR GENDER\n",
        "print(\"PREDICITING GENDER\")\n",
        "classifier = trainClassifier(trainData)\n",
        "testTrue = map(lambda t: t[1], testData)\n",
        "testPred = predictLabels(testData, classifier)\n",
        "finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted')\n",
        "finalScoresraw = precision_recall_fscore_support(testTrue, testPred)\n",
        "print \"Done training!\"\n",
        "print(\"Raw results for Gender = \", finalScoresraw)\n",
        "print \"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3]\n",
        "# print(labelMap)\n",
        "# print(labelMap1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICITING GENDER\n",
            "Training Classifier...\n",
            "Done training!\n",
            "('Raw results for Gender = ', (array([0.5625    , 0.61211129]), array([0.54857143, 0.62541806]), array([0.55544841, 0.61869313]), array([525, 598])))\n",
            "Precision: 0.588918\n",
            "Recall: 0.589492\n",
            "F Score:0.589126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Xk3G4ngQ1V",
        "colab_type": "code",
        "outputId": "275b11a3-9de6-4f3e-b3d5-da131396eb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "##### PREDICTION FOR SPEAKER\n",
        "print(\"PREDICTING SPEAKER\")\n",
        "classifier1 = trainClassifier(trainData1)\n",
        "testTrue1 = map(lambda t: t[1], testData1)\n",
        "testPred1 = predictLabels(testData1, classifier1)\n",
        "finalScores1 = precision_recall_fscore_support(testTrue1, testPred1, average='weighted')\n",
        "finalScoresraw1 = precision_recall_fscore_support(testTrue1, testPred1)\n",
        "\n",
        "print \"Done training!\"\n",
        "# print(\"Raw results for speaker = \", finalScoresraw1)\n",
        "print \"Precision1: %f\\nRecall1: %f\\nF Score1:%f\" % finalScores1[:3]\n",
        "# print(labelMap)\n",
        "# print(labelMap1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTING SPEAKER\n",
            "Training Classifier...\n",
            "Done training!\n",
            "Precision1: 0.231115\n",
            "Recall1: 0.204809\n",
            "F Score1:0.186925\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}